{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  MiniGPT Part 1: How GPT Reads Your Words\n",
    "\n",
    "**Understanding Tokenization - The Foundation of Large Language Models**\n",
    "\n",
    "Welcome to the first part of our MiniGPT series! In this notebook, you'll learn how GPT models \"read\" text by breaking it down into tokens - the fundamental building blocks that make modern AI possible.\n",
    "\n",
    "## ğŸ¯ What You'll Learn\n",
    "\n",
    "- **What tokenization is** and why it's crucial for LLMs\n",
    "- **How different tokenizers work** (character-level, word-level, subword)\n",
    "- **The famous \"strawberry problem\"** and why GPT can't count letters\n",
    "- **Byte-Pair Encoding (BPE)** - the algorithm used by GPT models\n",
    "- **Hands-on implementation** of your own tokenizers\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "\n",
    "**Option 1: Run in Google Colab (Recommended)**\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/naresh-sharma/mini-gpt/blob/main/notebooks/part1_tokenization.ipynb)\n",
    "\n",
    "**Option 2: Run Locally**\n",
    "```bash\n",
    "# Install MiniGPT\n",
    "pip install -e .\n",
    "\n",
    "# Start Jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š The Big Picture\n",
    "\n",
    "Before we dive into code, let's understand why tokenization matters:\n",
    "\n",
    "> **\"GPT doesn't see words. It sees tokens.\"**\n",
    "\n",
    "This simple statement explains so much about how modern AI works. Let's explore what this means!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MiniGPT already installed!\n",
      "ğŸ‰ Ready to explore tokenization!\n"
     ]
    }
   ],
   "source": [
    "# Install MiniGPT if running in Colab\n",
    "try:\n",
    "    import mini_gpt  # noqa: F401\n",
    "\n",
    "    print(\"âœ… MiniGPT already installed!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Installing MiniGPT...\")\n",
    "    !pip install -q git+https://github.com/naresh-sharma/mini-gpt.git\n",
    "    print(\"âœ… Installation complete!\")\n",
    "\n",
    "# Import our tokenizers and utilities\n",
    "from mini_gpt import BPETokenizer, SimpleTokenizer\n",
    "from mini_gpt.utils import (\n",
    "    analyze_text_efficiency,\n",
    "    compare_tokenizers,\n",
    "    demonstrate_strawberry_problem,\n",
    ")\n",
    "\n",
    "print(\"ğŸ‰ Ready to explore tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text into smaller pieces called **tokens**. Think of it as cutting a sentence into puzzle pieces that a computer can understand.\n",
    "\n",
    "### Why Do We Need Tokenization?\n",
    "\n",
    "1. **Computers don't understand text** - they only understand numbers\n",
    "2. **We need to convert text to numbers** - each token gets a unique ID\n",
    "3. **Different approaches** - character-level, word-level, or subword-level\n",
    "\n",
    "Let's see this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Original text: Hello world!\n",
      "ğŸ“Š Character count: 12\n",
      "ğŸ”¤ Characters: ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n",
      "\n",
      "ğŸ“š Vocabulary: {'Hello': 1, ' world': 2, '!': 3, '<UNK>': 0}\n",
      "\n",
      "ğŸ¯ Tokenized: [1, 2, 3]\n",
      "ğŸ”„ Decoded: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a simple example\n",
    "text = \"Hello world!\"\n",
    "\n",
    "print(\"ğŸ“ Original text:\", text)\n",
    "print(\"ğŸ“Š Character count:\", len(text))\n",
    "print(\"ğŸ”¤ Characters:\", list(text))\n",
    "\n",
    "# Create a simple vocabulary\n",
    "vocab = {\n",
    "    \"Hello\": 1,\n",
    "    \" world\": 2,  # Note the space at the beginning\n",
    "    \"!\": 3,\n",
    "    \"<UNK>\": 0,  # Unknown token\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“š Vocabulary:\", vocab)\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nğŸ¯ Tokenized: {tokens}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"ğŸ”„ Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ The Famous Strawberry Problem\n",
    "\n",
    "Now let's explore one of the most famous examples in AI: the \"strawberry problem.\" This perfectly illustrates why tokenization matters and why GPT models struggle with certain tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THE STRAWBERRY PROBLEM\n",
      "============================================================\n",
      "Why GPT can't count letters reliably...\n",
      "\n",
      "When you ask GPT 'How many R's are in strawberry?',\n",
      "it doesn't see individual letters. It sees tokens!\n",
      "\n",
      "You see:  s-t-r-a-w-b-e-r-r-y (10 letters, 3 R's)\n",
      "GPT sees: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>'] (10 tokens)\n",
      "\n",
      "GPT doesn't have direct access to the letters!\n",
      "It only knows about tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "This is why GPT models:\n",
      "  âŒ Struggle to count letters\n",
      "  âŒ Can't reliably spell backwards\n",
      "  âŒ Have difficulty with character-level tasks\n",
      "\n",
      "Modern models (like GPT-4+) learned to work around this\n",
      "through better reasoning, but tokenization still happens!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's demonstrate the strawberry problem\n",
    "demonstrate_strawberry_problem(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Building a Better Tokenizer: Byte-Pair Encoding\n",
    "\n",
    "The SimpleTokenizer we used above is too basic for real-world use. Let's build a more sophisticated tokenizer using **Byte-Pair Encoding (BPE)** - the same algorithm used by GPT models!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Training BPE tokenizer...\n",
      "Training BPE tokenizer on 6 texts...\n",
      "Target vocabulary size: 100\n",
      "Found 13 unique words\n",
      "Character-level vocab size: 33\n",
      "Training complete! Final vocab size: 87\n",
      "Number of merges: 54\n",
      "\n",
      "âœ… Training complete!\n",
      "ğŸ“š Vocabulary size: 87\n",
      "ğŸ”— Number of merges: 54\n"
     ]
    }
   ],
   "source": [
    "# Create a BPE tokenizer\n",
    "bpe_tokenizer = BPETokenizer(vocab_size=100)\n",
    "\n",
    "# Training data (in practice, this would be much larger)\n",
    "training_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Hello there!\",\n",
    "    \"The quick brown fox\",\n",
    "    \"strawberry pie\",\n",
    "    \"strawberry jam\",\n",
    "    \"I love strawberries\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Training BPE tokenizer...\")\n",
    "bpe_tokenizer.train(training_texts, verbose=True)\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"ğŸ“š Vocabulary size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "print(f\"ğŸ”— Number of merges: {len(bpe_tokenizer.get_merges())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Comparing tokenization approaches:\n",
      "==================================================\n",
      "SimpleTokenizer: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "BPETokenizer:    [47]\n",
      "\n",
      "ğŸ“Š Tokenization comparison for 'strawberry':\n",
      "================================================================================\n",
      "TOKENIZER COMPARISON\n",
      "================================================================================\n",
      "Text: \"strawberry\"\n",
      "\n",
      "Simple:\n",
      "  Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "  Count:  10 tokens\n",
      "\n",
      "BPE:\n",
      "  Tokens: ['ID:47']\n",
      "  Count:  1 token\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's compare our tokenizers!\n",
    "test_text = \"strawberry\"\n",
    "\n",
    "print(\"ğŸ” Comparing tokenization approaches:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simple tokenizer\n",
    "simple_tokens = tokenizer.encode(test_text)\n",
    "print(f\"SimpleTokenizer: {simple_tokens}\")\n",
    "\n",
    "# BPE tokenizer\n",
    "bpe_tokens = bpe_tokenizer.encode(test_text)\n",
    "print(f\"BPETokenizer:    {bpe_tokens}\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(f\"\\nğŸ“Š Tokenization comparison for '{test_text}':\")\n",
    "compare_tokenizers(test_text, [tokenizer, bpe_tokenizer], [\"Simple\", \"BPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Efficiency Analysis\n",
    "\n",
    "Let's analyze how efficiently our tokenizers compress text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Tokenization Efficiency Analysis\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Text: 'Hello world!'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 4.00 chars/token\n",
      "BPETokenizer:    6.00 chars/token\n",
      "ğŸ† SimpleTokenizer is more efficient\n",
      "\n",
      "ğŸ“ Text: 'strawberry'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    10.00 chars/token\n",
      "ğŸ† SimpleTokenizer is more efficient\n",
      "\n",
      "ğŸ“ Text: 'The quick brown fox jumps over the lazy dog.'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    1.76 chars/token\n",
      "ğŸ† SimpleTokenizer is more efficient\n",
      "\n",
      "ğŸ“ Text: 'I love programming in Python!'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    1.21 chars/token\n",
      "ğŸ† SimpleTokenizer is more efficient\n"
     ]
    }
   ],
   "source": [
    "# Analyze efficiency for different texts\n",
    "test_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"strawberry\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I love programming in Python!\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Tokenization Efficiency Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nğŸ“ Text: '{text}'\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Simple tokenizer efficiency\n",
    "    simple_metrics = analyze_text_efficiency(text, tokenizer)\n",
    "    print(f\"SimpleTokenizer: {simple_metrics['chars_per_token']:.2f} chars/token\")\n",
    "\n",
    "    # BPE tokenizer efficiency\n",
    "    bpe_metrics = analyze_text_efficiency(text, bpe_tokenizer)\n",
    "    print(f\"BPETokenizer:    {bpe_metrics['chars_per_token']:.2f} chars/token\")\n",
    "\n",
    "    # Which is more efficient?\n",
    "    if bpe_metrics[\"chars_per_token\"] > simple_metrics[\"chars_per_token\"]:\n",
    "        print(\"ğŸ† SimpleTokenizer is more efficient\")\n",
    "    else:\n",
    "        print(\"ğŸ† BPETokenizer is more efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Tokenization is fundamental** - Every LLM starts with tokenization\n",
    "2. **Different approaches exist** - Character, word, and subword tokenization\n",
    "3. **BPE is powerful** - It learns common patterns and creates efficient representations\n",
    "4. **The strawberry problem** - Shows why GPT struggles with character-level tasks\n",
    "5. **Efficiency matters** - Better tokenization = better model performance\n",
    "\n",
    "### Why This Matters for GPT\n",
    "\n",
    "- **GPT-3 uses BPE** with ~50,000 tokens\n",
    "- **Each token** gets converted to a vector (embedding)\n",
    "- **The model learns** relationships between these vectors\n",
    "- **Better tokenization** = better understanding of language\n",
    "\n",
    "## ğŸš€ What's Next?\n",
    "\n",
    "In **Part 2: Embeddings**, we'll learn how tokens become vectors that capture meaning!\n",
    "\n",
    "- How do we convert tokens to numbers?\n",
    "- What are embeddings and why do they matter?\n",
    "- How does GPT understand relationships between words?\n",
    "\n",
    "**Ready to continue?** Check out the next notebook or explore the code further!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Additional Resources\n",
    "\n",
    "- [Full MiniGPT Repository](https://github.com/naresh-sharma/mini-gpt)\n",
    "- [Part 2: Embeddings Notebook](notebooks/part2_embeddings.ipynb) (Coming Soon)\n",
    "- [Blog Post: How GPT Reads Your Words](your-blog-link) (Coming Soon)\n",
    "\n",
    "**Happy tokenizing!** ğŸ‰\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
