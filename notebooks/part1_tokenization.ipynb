{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† MiniGPT Part 1: How GPT Reads Your Words\n",
    "\n",
    "**Understanding Tokenization - The Foundation of Large Language Models**\n",
    "\n",
    "Welcome to the first part of our MiniGPT series! In this notebook, you'll learn how GPT models \"read\" text by breaking it down into tokens - the fundamental building blocks that make modern AI possible.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "- **What tokenization is** and why it's crucial for LLMs\n",
    "- **How different tokenizers work** (character-level, word-level, subword)\n",
    "- **The famous \"strawberry problem\"** and why GPT can't count letters\n",
    "- **Byte-Pair Encoding (BPE)** - the algorithm used by GPT models\n",
    "- **Hands-on implementation** of your own tokenizers\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "**Option 1: Run in Google Colab (Recommended)**\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/naresh-sharma/mini-gpt/blob/main/notebooks/part1_tokenization.ipynb)\n",
    "\n",
    "**Option 2: Run Locally**\n",
    "```bash\n",
    "# Navigate to the project root directory (where setup.py is located)\n",
    "cd /path/to/mini-gpt\n",
    "\n",
    "# Install MiniGPT\n",
    "pip install -e .\n",
    "\n",
    "# Start Jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**Important:** After Jupyter opens in your browser:\n",
    "1. Navigate to the `notebooks/` folder in the Jupyter file browser\n",
    "2. Open `part1_tokenization.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "## üìö The Big Picture\n",
    "Before we dive into code, let's understand why tokenization matters:\n",
    "\n",
    "> **\"GPT doesn't see words. It sees tokens.\"**\n",
    "\n",
    "This simple statement explains so much about how modern AI works. Let's explore what this means!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MiniGPT already installed!\n",
      "üéâ Ready to explore tokenization!\n"
     ]
    }
   ],
   "source": [
    "# Install MiniGPT if running in Colab\n",
    "try:\n",
    "    import mini_gpt  # noqa: F401\n",
    "\n",
    "    print(\"‚úÖ MiniGPT already installed!\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing MiniGPT...\")\n",
    "    %pip install -q git+https://github.com/naresh-sharma/mini-gpt.git\n",
    "    print(\"‚úÖ Installation complete!\")\n",
    "\n",
    "# Install additional dependencies for visualization\n",
    "%pip install -q matplotlib\n",
    "\n",
    "# Import our tokenizers and utilities\n",
    "from mini_gpt import BPETokenizer, SimpleTokenizer\n",
    "from mini_gpt.utils import (\n",
    "    analyze_text_efficiency,\n",
    "    compare_tokenizers,\n",
    "    demonstrate_strawberry_problem,\n",
    ")\n",
    "\n",
    "print(\"üéâ Ready to explore tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text into smaller pieces called **tokens**. Think of it as cutting a sentence into puzzle pieces that a computer can understand.\n",
    "\n",
    "### Why Do We Need Tokenization?\n",
    "\n",
    "1. **Computers don't understand text** - they only understand numbers\n",
    "2. **We need to convert text to numbers** - each token gets a unique ID\n",
    "3. **Different approaches** - character-level, word-level, or subword-level\n",
    "\n",
    "Let's see this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original text: Hello world!\n",
      "üìä Character count: 12\n",
      "üî§ Characters: ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n",
      "\n",
      "üìö Vocabulary: {'Hello': 1, ' world': 2, '!': 3, '<UNK>': 0}\n",
      "\n",
      "üéØ Tokenized: [1, 2, 3]\n",
      "üîÑ Decoded: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a simple example\n",
    "text = \"Hello world!\"\n",
    "\n",
    "print(\"üìù Original text:\", text)\n",
    "print(\"üìä Character count:\", len(text))\n",
    "print(\"üî§ Characters:\", list(text))\n",
    "\n",
    "# Create a simple vocabulary\n",
    "vocab = {\n",
    "    \"Hello\": 1,\n",
    "    \" world\": 2,  # Note the space at the beginning\n",
    "    \"!\": 3,\n",
    "    \"<UNK>\": 0,  # Unknown token\n",
    "}\n",
    "\n",
    "print(\"\\nüìö Vocabulary:\", vocab)\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nüéØ Tokenized: {tokens}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"üîÑ Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üçì The Famous Strawberry Problem\n",
    "\n",
    "Now let's explore one of the most famous examples in AI: the \"strawberry problem.\" This perfectly illustrates why tokenization matters and why GPT models struggle with certain tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THE STRAWBERRY PROBLEM\n",
      "============================================================\n",
      "Why GPT can't count letters reliably...\n",
      "\n",
      "When you ask GPT 'How many R's are in strawberry?',\n",
      "it doesn't see individual letters. It sees tokens!\n",
      "\n",
      "You see:  s-t-r-a-w-b-e-r-r-y (10 letters, 3 R's)\n",
      "GPT sees: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>'] (10 tokens)\n",
      "\n",
      "GPT doesn't have direct access to the letters!\n",
      "It only knows about tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "This is why GPT models:\n",
      "  ‚ùå Struggle to count letters\n",
      "  ‚ùå Can't reliably spell backwards\n",
      "  ‚ùå Have difficulty with character-level tasks\n",
      "\n",
      "Modern models (like GPT-4+) learned to work around this\n",
      "through better reasoning, but tokenization still happens!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's demonstrate the strawberry problem\n",
    "demonstrate_strawberry_problem(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Try it yourself! Experiment with BPE\n",
    "# Try training the BPE tokenizer on your own text data\n",
    "\n",
    "from mini_gpt.utils import visualize_tokens\n",
    "\n",
    "# Your turn! Replace this with your own text\n",
    "your_texts = [\n",
    "    \"Replace me with your own sentences\",\n",
    "    \"Try different types of text\",\n",
    "    \"See how BPE learns patterns from your data\",\n",
    "]\n",
    "\n",
    "print(\"üéØ Training BPE on your custom text...\")\n",
    "custom_bpe = BPETokenizer(vocab_size=100)  # Minimum vocab size for quick training\n",
    "custom_bpe.train(your_texts, verbose=True)\n",
    "\n",
    "print(\"\\nüß™ Now test it on new text:\")\n",
    "test_text = \"Try this sentence with your trained tokenizer\"\n",
    "tokens = custom_bpe.encode(test_text)\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "\n",
    "# Visualize the tokenization\n",
    "visualize_tokens(test_text, custom_bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Building a Better Tokenizer: Byte-Pair Encoding\n",
    "\n",
    "The SimpleTokenizer we used above is too basic for real-world use. Let's build a more sophisticated tokenizer using **Byte-Pair Encoding (BPE)** - the same algorithm used by GPT models!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Training BPE tokenizer...\n",
      "Training BPE tokenizer on 6 texts...\n",
      "Target vocabulary size: 100\n",
      "Found 13 unique words\n",
      "Character-level vocab size: 33\n",
      "Training complete! Final vocab size: 87\n",
      "Number of merges: 54\n",
      "\n",
      "‚úÖ Training complete!\n",
      "üìö Vocabulary size: 87\n",
      "üîó Number of merges: 54\n"
     ]
    }
   ],
   "source": [
    "# Create a BPE tokenizer\n",
    "bpe_tokenizer = BPETokenizer(vocab_size=100)\n",
    "\n",
    "# Training data (in practice, this would be much larger)\n",
    "training_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Hello there!\",\n",
    "    \"The quick brown fox\",\n",
    "    \"strawberry pie\",\n",
    "    \"strawberry jam\",\n",
    "    \"I love strawberries\",\n",
    "]\n",
    "\n",
    "print(\"üéì Training BPE tokenizer...\")\n",
    "bpe_tokenizer.train(training_texts, verbose=True)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"üìö Vocabulary size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "print(f\"üîó Number of merges: {len(bpe_tokenizer.get_merges())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Test texts for visualization\u001b[39;00m\n\u001b[32m      5\u001b[39m texts = [\u001b[33m\"\u001b[39m\u001b[33mHello world!\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstrawberry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPython programming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m simple_counts = [\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m.encode(t)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m      7\u001b[39m bpe_counts = [\u001b[38;5;28mlen\u001b[39m(bpe_tokenizer.encode(t)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create horizontal bar chart\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# üñºÔ∏è Visual Comparison: Token Counts per Text\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test texts for visualization\n",
    "texts = [\"Hello world!\", \"strawberry\", \"Python programming\"]\n",
    "simple_counts = [len(tokenizer.encode(t)) for t in texts]\n",
    "bpe_counts = [len(bpe_tokenizer.encode(t)) for t in texts]\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_pos = range(len(texts))\n",
    "\n",
    "# Plot SimpleTokenizer bars\n",
    "bars1 = ax.barh(y_pos, simple_counts, label=\"SimpleTokenizer\", alpha=0.8, color=\"skyblue\")\n",
    "\n",
    "# Plot BPE bars (stacked on top of SimpleTokenizer)\n",
    "bars2 = ax.barh(\n",
    "    y_pos, bpe_counts, left=simple_counts, label=\"BPETokenizer\", alpha=0.8, color=\"lightcoral\"\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(texts)\n",
    "ax.set_xlabel(\"Token Count\")\n",
    "ax.set_title(\"Tokenization Comparison: SimpleTokenizer vs BPETokenizer\")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (simple, bpe) in enumerate(zip(simple_counts, bpe_counts)):\n",
    "    ax.text(simple / 2, i, str(simple), ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "    ax.text(simple + bpe / 2, i, str(bpe), ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Insights:\")\n",
    "print(\"‚Ä¢ Lower bars = more efficient tokenization\")\n",
    "print(\"‚Ä¢ SimpleTokenizer often uses fewer tokens for short texts\")\n",
    "print(\"‚Ä¢ BPE learns patterns and can be more efficient for longer texts\")\n",
    "print(\"‚Ä¢ The difference shows why tokenization strategy matters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Try it yourself! Test the strawberry problem\n",
    "# Try different words and see how they get tokenized\n",
    "\n",
    "# Your turn! Test these words or add your own\n",
    "test_words = [\n",
    "    \"strawberry\",  # The classic example\n",
    "    \"programming\",  # Try a longer word\n",
    "    \"hello\",  # Try a simple word\n",
    "    \"supercalifragilisticexpialidocious\",  # Try a very long word\n",
    "]\n",
    "\n",
    "print(\"üîç Testing different words with tokenization:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"\\nüìù Word: '{word}'\")\n",
    "    print(f\"   Letters: {len(word)}\")\n",
    "\n",
    "    # Test with SimpleTokenizer\n",
    "    simple_tokens = tokenizer.encode(word)\n",
    "    print(f\"   SimpleTokenizer: {len(simple_tokens)} tokens\")\n",
    "\n",
    "    # Test with BPE\n",
    "    bpe_tokens = bpe_tokenizer.encode(word)\n",
    "    print(f\"   BPETokenizer: {len(bpe_tokens)} tokens\")\n",
    "\n",
    "    # Show the actual tokens\n",
    "    print(f\"   Simple tokens: {simple_tokens}\")\n",
    "    print(f\"   BPE tokens: {bpe_tokens}\")\n",
    "\n",
    "print(\n",
    "    \"\\nüí° Key insight: The more tokens a word becomes, the harder it is for GPT to 'see' the individual letters!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compare our tokenizers!\n",
    "test_text = \"strawberry\"\n",
    "\n",
    "print(\"üîç Comparing tokenization approaches:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simple tokenizer\n",
    "simple_tokens = tokenizer.encode(test_text)\n",
    "print(f\"SimpleTokenizer: {simple_tokens}\")\n",
    "\n",
    "# BPE tokenizer\n",
    "bpe_tokens = bpe_tokenizer.encode(test_text)\n",
    "print(f\"BPETokenizer:    {bpe_tokens}\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(f\"\\nüìä Tokenization comparison for '{test_text}':\")\n",
    "compare_tokenizers(test_text, [tokenizer, bpe_tokenizer], [\"Simple\", \"BPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Efficiency Analysis\n",
    "\n",
    "Let's analyze how efficiently our tokenizers compress text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Tokenization Efficiency Analysis\n",
      "============================================================\n",
      "\n",
      "üìù Text: 'Hello world!'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 4.00 chars/token\n",
      "BPETokenizer:    6.00 chars/token\n",
      "üèÜ SimpleTokenizer is more efficient\n",
      "\n",
      "üìù Text: 'strawberry'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    10.00 chars/token\n",
      "üèÜ SimpleTokenizer is more efficient\n",
      "\n",
      "üìù Text: 'The quick brown fox jumps over the lazy dog.'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    1.76 chars/token\n",
      "üèÜ SimpleTokenizer is more efficient\n",
      "\n",
      "üìù Text: 'I love programming in Python!'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    1.21 chars/token\n",
      "üèÜ SimpleTokenizer is more efficient\n"
     ]
    }
   ],
   "source": [
    "# Analyze efficiency for different texts\n",
    "test_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"strawberry\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I love programming in Python!\",\n",
    "]\n",
    "\n",
    "print(\"üìä Tokenization Efficiency Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nüìù Text: '{text}'\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Simple tokenizer efficiency\n",
    "    simple_metrics = analyze_text_efficiency(text, tokenizer)\n",
    "    print(f\"SimpleTokenizer: {simple_metrics['chars_per_token']:.2f} chars/token\")\n",
    "\n",
    "    # BPE tokenizer efficiency\n",
    "    bpe_metrics = analyze_text_efficiency(text, bpe_tokenizer)\n",
    "    print(f\"BPETokenizer:    {bpe_metrics['chars_per_token']:.2f} chars/token\")\n",
    "\n",
    "    # Which is more efficient?\n",
    "    if bpe_metrics[\"chars_per_token\"] > simple_metrics[\"chars_per_token\"]:\n",
    "        print(\"üèÜ SimpleTokenizer is more efficient\")\n",
    "    else:\n",
    "        print(\"üèÜ BPETokenizer is more efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Tokenization is fundamental** - Every LLM starts with tokenization\n",
    "2. **Different approaches exist** - Character, word, and subword tokenization\n",
    "3. **BPE is powerful** - It learns common patterns and creates efficient representations\n",
    "4. **The strawberry problem** - Shows why GPT struggles with character-level tasks\n",
    "5. **Efficiency matters** - Better tokenization = better model performance\n",
    "\n",
    "### Why This Matters for GPT\n",
    "\n",
    "- **GPT-3 uses BPE** with ~50,000 tokens\n",
    "- **Each token** gets converted to a vector (embedding)\n",
    "- **The model learns** relationships between these vectors\n",
    "- **Better tokenization** = better understanding of language\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "In **Part 2: Embeddings**, we'll learn how tokens become vectors that capture meaning!\n",
    "\n",
    "- How do we convert tokens to numbers?\n",
    "- What are embeddings and why do they matter?\n",
    "- How does GPT understand relationships between words?\n",
    "\n",
    "**Ready to continue?** Check out the next notebook or explore the code further!\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Additional Resources\n",
    "\n",
    "- [Full MiniGPT Repository](https://github.com/naresh-sharma/mini-gpt)\n",
    "- [Part 2: Embeddings Notebook](notebooks/part2_embeddings.ipynb) (Coming Soon)\n",
    "- [Blog Post: How GPT Reads Your Words](your-blog-link) (Coming Soon)\n",
    "\n",
    "**Happy tokenizing!** üéâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Your Tokenization Experiment\n",
      "==================================================\n",
      "Text: 'Replace me with anything you want to explore!'\n",
      "\n",
      "SimpleTokenizer: 45 tokens\n",
      "BPETokenizer: 42 tokens\n",
      "\n",
      "üîç Tokenization breakdown:\n",
      "\n",
      "SimpleTokenizer:\n",
      "============================================================\n",
      "TOKENIZATION VISUALIZATION\n",
      "============================================================\n",
      "Original: \"Replace me with anything you want to explore!\"\n",
      "Tokens:   ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '!']\n",
      "IDs:      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
      "Count:    45 tokens\n",
      "============================================================\n",
      "\n",
      "BPETokenizer:\n",
      "============================================================\n",
      "TOKENIZATION VISUALIZATION\n",
      "============================================================\n",
      "Original: \"Replace me with anything you want to explore!\"\n",
      "Tokens:   ['ID:32', 'ID:0', 'ID:12', 'ID:22', 'ID:18', 'ID:8', 'ID:10', 'ID:12', 'ID:32', 'ID:19', 'ID:12', 'ID:49', 'ID:15', 'ID:26', 'ID:14', 'ID:32', 'ID:8', 'ID:20', 'ID:31', 'ID:26', 'ID:14', 'ID:15', 'ID:20', 'ID:0', 'ID:32', 'ID:31', 'ID:21', 'ID:27', 'ID:49', 'ID:8', 'ID:20', 'ID:26', 'ID:55', 'ID:21', 'ID:32', 'ID:12', 'ID:30', 'ID:22', 'ID:34', 'ID:24', 'ID:12', 'ID:4']\n",
      "IDs:      [32, 0, 12, 22, 18, 8, 10, 12, 32, 19, 12, 49, 15, 26, 14, 32, 8, 20, 31, 26, 14, 15, 20, 0, 32, 31, 21, 27, 49, 8, 20, 26, 55, 21, 32, 12, 30, 22, 34, 24, 12, 4]\n",
      "Count:    42 tokens\n",
      "============================================================\n",
      "\n",
      "üéâ Experiment complete! Try different texts to see how tokenization changes!\n"
     ]
    }
   ],
   "source": [
    "# üß© Final Challenge: Your Own Tokenization Experiment\n",
    "# This is your playground! Try anything you want to explore\n",
    "\n",
    "from mini_gpt.utils import visualize_tokens\n",
    "\n",
    "# Replace this with your own text\n",
    "your_text = \"Replace me with anything you want to explore!\"\n",
    "\n",
    "print(\"üéØ Your Tokenization Experiment\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: '{your_text}'\")\n",
    "print()\n",
    "\n",
    "# Test with both tokenizers\n",
    "simple_tokens = tokenizer.encode(your_text)\n",
    "bpe_tokens = bpe_tokenizer.encode(your_text)\n",
    "\n",
    "print(f\"SimpleTokenizer: {len(simple_tokens)} tokens\")\n",
    "print(f\"BPETokenizer: {len(bpe_tokens)} tokens\")\n",
    "print()\n",
    "\n",
    "# Visualize the tokenization\n",
    "print(\"üîç Tokenization breakdown:\")\n",
    "\n",
    "print(\"\\nSimpleTokenizer:\")\n",
    "visualize_tokens(your_text, tokenizer)\n",
    "\n",
    "print(\"\\nBPETokenizer:\")\n",
    "visualize_tokens(your_text, bpe_tokenizer)\n",
    "\n",
    "print(\"\\nüéâ Experiment complete! Try different texts to see how tokenization changes!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
