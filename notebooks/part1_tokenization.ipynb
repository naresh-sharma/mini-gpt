{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 MiniGPT Part 1: How GPT Reads Your Words\n",
    "\n",
    "**Understanding Tokenization - The Foundation of Large Language Models**\n",
    "\n",
    "Welcome to the first part of our MiniGPT series! In this notebook, you'll learn how GPT models \"read\" text by breaking it down into tokens - the fundamental building blocks that make modern AI possible.\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "- **What tokenization is** and why it's crucial for LLMs\n",
    "- **How different tokenizers work** (character-level, word-level, subword)\n",
    "- **The famous \"strawberry problem\"** and why GPT can't count letters\n",
    "- **Byte-Pair Encoding (BPE)** - the algorithm used by GPT models\n",
    "- **Hands-on implementation** of your own tokenizers\n",
    "\n",
    "## 🚀 Quick Start\n",
    "\n",
    "**Option 1: Run in Google Colab (Recommended)**\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/naresh-sharma/mini-gpt/blob/main/notebooks/part1_tokenization.ipynb)\n",
    "\n",
    "**Option 2: Run Locally**\n",
    "```bash\n",
    "# Navigate to the project root directory (where setup.py is located)\n",
    "cd /path/to/mini-gpt\n",
    "\n",
    "# Install MiniGPT\n",
    "pip install -e .\n",
    "\n",
    "# Start Jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**Important:** After Jupyter opens in your browser:\n",
    "1. Navigate to the `notebooks/` folder in the Jupyter file browser\n",
    "2. Open `part1_tokenization.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 The Big Picture\n",
    "Before we dive into code, let's understand why tokenization matters:\n",
    "\n",
    "> **\"GPT doesn't see words. It sees tokens.\"**\n",
    "\n",
    "This simple statement explains so much about how modern AI works. Let's explore what this means!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MiniGPT already installed!\n",
      "🎉 Ready to explore tokenization!\n"
     ]
    }
   ],
   "source": [
    "# Install MiniGPT if running in Colab\n",
    "try:\n",
    "    import mini_gpt  # noqa: F401\n",
    "\n",
    "    print(\"✅ MiniGPT already installed!\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing MiniGPT...\")\n",
    "    %pip install -q git+https://github.com/naresh-sharma/mini-gpt.git\n",
    "    print(\"✅ Installation complete!\")\n",
    "\n",
    "# Install additional dependencies for visualization\n",
    "%pip install -q matplotlib\n",
    "\n",
    "# Import our tokenizers and utilities\n",
    "from mini_gpt import BPETokenizer, SimpleTokenizer\n",
    "from mini_gpt.utils import (\n",
    "    analyze_text_efficiency,\n",
    "    compare_tokenizers,\n",
    "    demonstrate_strawberry_problem,\n",
    ")\n",
    "\n",
    "print(\"🎉 Ready to explore tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text into smaller pieces called **tokens**. Think of it as cutting a sentence into puzzle pieces that a computer can understand.\n",
    "\n",
    "### Why Do We Need Tokenization?\n",
    "\n",
    "1. **Computers don't understand text** - they only understand numbers\n",
    "2. **We need to convert text to numbers** - each token gets a unique ID\n",
    "3. **Different approaches** - character-level, word-level, or subword-level\n",
    "\n",
    "Let's see this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Original text: Hello world!\n",
      "📊 Character count: 12\n",
      "🔤 Characters: ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n",
      "\n",
      "📚 Vocabulary: {'Hello': 1, ' world': 2, '!': 3, '<UNK>': 0}\n",
      "\n",
      "🎯 Tokenized: [1, 2, 3]\n",
      "🔄 Decoded: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a simple example\n",
    "text = \"Hello world!\"\n",
    "\n",
    "print(\"📝 Original text:\", text)\n",
    "print(\"📊 Character count:\", len(text))\n",
    "print(\"🔤 Characters:\", list(text))\n",
    "\n",
    "# Create a simple vocabulary\n",
    "vocab = {\n",
    "    \"Hello\": 1,\n",
    "    \" world\": 2,  # Note the space at the beginning\n",
    "    \"!\": 3,\n",
    "    \"<UNK>\": 0,  # Unknown token\n",
    "}\n",
    "\n",
    "print(\"\\n📚 Vocabulary:\", vocab)\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\n🎯 Tokenized: {tokens}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"🔄 Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍓 The Famous Strawberry Problem\n",
    "\n",
    "Now let's explore one of the most famous examples in AI: the \"strawberry problem.\" This perfectly illustrates why tokenization matters and why GPT models struggle with certain tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THE STRAWBERRY PROBLEM\n",
      "============================================================\n",
      "Why GPT can't count letters reliably...\n",
      "\n",
      "When you ask GPT 'How many R's are in strawberry?',\n",
      "it doesn't see individual letters. It sees tokens!\n",
      "\n",
      "You see:  s-t-r-a-w-b-e-r-r-y (10 letters, 3 R's)\n",
      "GPT sees: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>'] (10 tokens)\n",
      "\n",
      "GPT doesn't have direct access to the letters!\n",
      "It only knows about tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "This is why GPT models:\n",
      "  ❌ Struggle to count letters\n",
      "  ❌ Can't reliably spell backwards\n",
      "  ❌ Have difficulty with character-level tasks\n",
      "\n",
      "Modern models (like GPT-4+) learned to work around this\n",
      "through better reasoning, but tokenization still happens!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's demonstrate the strawberry problem\n",
    "demonstrate_strawberry_problem(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧩 Try it yourself! Experiment with BPE\n",
    "# Try training the BPE tokenizer on your own text data\n",
    "\n",
    "from mini_gpt.utils import visualize_tokens\n",
    "\n",
    "# Your turn! Replace this with your own text\n",
    "your_texts = [\n",
    "    \"Replace me with your own sentences\",\n",
    "    \"Try different types of text\",\n",
    "    \"See how BPE learns patterns from your data\",\n",
    "]\n",
    "\n",
    "print(\"🎯 Training BPE on your custom text...\")\n",
    "custom_bpe = BPETokenizer(vocab_size=100)  # Minimum vocab size for quick training\n",
    "custom_bpe.train(your_texts, verbose=True)\n",
    "\n",
    "print(\"\\n🧪 Now test it on new text:\")\n",
    "test_text = \"Try this sentence with your trained tokenizer\"\n",
    "tokens = custom_bpe.encode(test_text)\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "\n",
    "# Visualize the tokenization\n",
    "visualize_tokens(test_text, custom_bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Building a Better Tokenizer: Byte-Pair Encoding\n",
    "\n",
    "The SimpleTokenizer we used above is too basic for real-world use. Let's build a more sophisticated tokenizer using **Byte-Pair Encoding (BPE)** - the same algorithm used by GPT models!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 Training BPE tokenizer...\n",
      "Training BPE tokenizer on 6 texts...\n",
      "Target vocabulary size: 100\n",
      "Found 13 unique words\n",
      "Character-level vocab size: 33\n",
      "Training complete! Final vocab size: 87\n",
      "Number of merges: 54\n",
      "\n",
      "✅ Training complete!\n",
      "📚 Vocabulary size: 87\n",
      "🔗 Number of merges: 54\n"
     ]
    }
   ],
   "source": [
    "# Create a BPE tokenizer\n",
    "bpe_tokenizer = BPETokenizer(vocab_size=100)\n",
    "\n",
    "# Training data (in practice, this would be much larger)\n",
    "training_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Hello there!\",\n",
    "    \"The quick brown fox\",\n",
    "    \"strawberry pie\",\n",
    "    \"strawberry jam\",\n",
    "    \"I love strawberries\",\n",
    "]\n",
    "\n",
    "print(\"🎓 Training BPE tokenizer...\")\n",
    "bpe_tokenizer.train(training_texts, verbose=True)\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "print(f\"📚 Vocabulary size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "print(f\"🔗 Number of merges: {len(bpe_tokenizer.get_merges())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Test texts for visualization\u001b[39;00m\n\u001b[32m      5\u001b[39m texts = [\u001b[33m\"\u001b[39m\u001b[33mHello world!\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstrawberry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPython programming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m simple_counts = [\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m.encode(t)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m      7\u001b[39m bpe_counts = [\u001b[38;5;28mlen\u001b[39m(bpe_tokenizer.encode(t)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create horizontal bar chart\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 🖼️ Visual Comparison: Token Counts per Text\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test texts for visualization\n",
    "texts = [\"Hello world!\", \"strawberry\", \"Python programming\"]\n",
    "simple_counts = [len(tokenizer.encode(t)) for t in texts]\n",
    "bpe_counts = [len(bpe_tokenizer.encode(t)) for t in texts]\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_pos = range(len(texts))\n",
    "\n",
    "# Plot SimpleTokenizer bars\n",
    "bars1 = ax.barh(y_pos, simple_counts, label=\"SimpleTokenizer\", alpha=0.8, color=\"skyblue\")\n",
    "\n",
    "# Plot BPE bars (stacked on top of SimpleTokenizer)\n",
    "bars2 = ax.barh(\n",
    "    y_pos, bpe_counts, left=simple_counts, label=\"BPETokenizer\", alpha=0.8, color=\"lightcoral\"\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(texts)\n",
    "ax.set_xlabel(\"Token Count\")\n",
    "ax.set_title(\"Tokenization Comparison: SimpleTokenizer vs BPETokenizer\")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (simple, bpe) in enumerate(zip(simple_counts, bpe_counts)):\n",
    "    ax.text(simple / 2, i, str(simple), ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "    ax.text(simple + bpe / 2, i, str(bpe), ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Key Insights:\")\n",
    "print(\"• Lower bars = more efficient tokenization\")\n",
    "print(\"• SimpleTokenizer often uses fewer tokens for short texts\")\n",
    "print(\"• BPE learns patterns and can be more efficient for longer texts\")\n",
    "print(\"• The difference shows why tokenization strategy matters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧩 Try it yourself! Test the strawberry problem\n",
    "# Try different words and see how they get tokenized\n",
    "\n",
    "# Your turn! Test these words or add your own\n",
    "test_words = [\n",
    "    \"strawberry\",  # The classic example\n",
    "    \"programming\",  # Try a longer word\n",
    "    \"hello\",  # Try a simple word\n",
    "    \"supercalifragilisticexpialidocious\",  # Try a very long word\n",
    "]\n",
    "\n",
    "print(\"🔍 Testing different words with tokenization:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"\\n📝 Word: '{word}'\")\n",
    "    print(f\"   Letters: {len(word)}\")\n",
    "\n",
    "    # Test with SimpleTokenizer\n",
    "    simple_tokens = tokenizer.encode(word)\n",
    "    print(f\"   SimpleTokenizer: {len(simple_tokens)} tokens\")\n",
    "\n",
    "    # Test with BPE\n",
    "    bpe_tokens = bpe_tokenizer.encode(word)\n",
    "    print(f\"   BPETokenizer: {len(bpe_tokens)} tokens\")\n",
    "\n",
    "    # Show the actual tokens\n",
    "    print(f\"   Simple tokens: {simple_tokens}\")\n",
    "    print(f\"   BPE tokens: {bpe_tokens}\")\n",
    "\n",
    "print(\n",
    "    \"\\n💡 Key insight: The more tokens a word becomes, the harder it is for GPT to 'see' the individual letters!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compare our tokenizers!\n",
    "test_text = \"strawberry\"\n",
    "\n",
    "print(\"🔍 Comparing tokenization approaches:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simple tokenizer\n",
    "simple_tokens = tokenizer.encode(test_text)\n",
    "print(f\"SimpleTokenizer: {simple_tokens}\")\n",
    "\n",
    "# BPE tokenizer\n",
    "bpe_tokens = bpe_tokenizer.encode(test_text)\n",
    "print(f\"BPETokenizer:    {bpe_tokens}\")\n",
    "\n",
    "# Visualize the differences\n",
    "print(f\"\\n📊 Tokenization comparison for '{test_text}':\")\n",
    "compare_tokenizers(test_text, [tokenizer, bpe_tokenizer], [\"Simple\", \"BPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Efficiency Analysis\n",
    "\n",
    "Let's analyze how efficiently our tokenizers compress text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Tokenization Efficiency Analysis\n",
      "============================================================\n",
      "\n",
      "📝 Text: 'Hello world!'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 4.00 chars/token\n",
      "BPETokenizer:    6.00 chars/token\n",
      "🏆 SimpleTokenizer is more efficient\n",
      "\n",
      "📝 Text: 'strawberry'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    10.00 chars/token\n",
      "🏆 SimpleTokenizer is more efficient\n",
      "\n",
      "📝 Text: 'The quick brown fox jumps over the lazy dog.'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    1.76 chars/token\n",
      "🏆 SimpleTokenizer is more efficient\n",
      "\n",
      "📝 Text: 'I love programming in Python!'\n",
      "----------------------------------------\n",
      "SimpleTokenizer: 1.00 chars/token\n",
      "BPETokenizer:    1.21 chars/token\n",
      "🏆 SimpleTokenizer is more efficient\n"
     ]
    }
   ],
   "source": [
    "# Analyze efficiency for different texts\n",
    "test_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"strawberry\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I love programming in Python!\",\n",
    "]\n",
    "\n",
    "print(\"📊 Tokenization Efficiency Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\n📝 Text: '{text}'\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Simple tokenizer efficiency\n",
    "    simple_metrics = analyze_text_efficiency(text, tokenizer)\n",
    "    print(f\"SimpleTokenizer: {simple_metrics['chars_per_token']:.2f} chars/token\")\n",
    "\n",
    "    # BPE tokenizer efficiency\n",
    "    bpe_metrics = analyze_text_efficiency(text, bpe_tokenizer)\n",
    "    print(f\"BPETokenizer:    {bpe_metrics['chars_per_token']:.2f} chars/token\")\n",
    "\n",
    "    # Which is more efficient?\n",
    "    if bpe_metrics[\"chars_per_token\"] > simple_metrics[\"chars_per_token\"]:\n",
    "        print(\"🏆 SimpleTokenizer is more efficient\")\n",
    "    else:\n",
    "        print(\"🏆 BPETokenizer is more efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Tokenization is fundamental** - Every LLM starts with tokenization\n",
    "2. **Different approaches exist** - Character, word, and subword tokenization\n",
    "3. **BPE is powerful** - It learns common patterns and creates efficient representations\n",
    "4. **The strawberry problem** - Shows why GPT struggles with character-level tasks\n",
    "5. **Efficiency matters** - Better tokenization = better model performance\n",
    "\n",
    "### Why This Matters for GPT\n",
    "\n",
    "- **GPT-3 uses BPE** with ~50,000 tokens\n",
    "- **Each token** gets converted to a vector (embedding)\n",
    "- **The model learns** relationships between these vectors\n",
    "- **Better tokenization** = better understanding of language\n",
    "\n",
    "## 🚀 What's Next?\n",
    "\n",
    "In **Part 2: Embeddings**, we'll learn how tokens become vectors that capture meaning!\n",
    "\n",
    "- How do we convert tokens to numbers?\n",
    "- What are embeddings and why do they matter?\n",
    "- How does GPT understand relationships between words?\n",
    "\n",
    "**Ready to continue?** Check out the next notebook or explore the code further!\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 Additional Resources\n",
    "\n",
    "- [Full MiniGPT Repository](https://github.com/naresh-sharma/mini-gpt)\n",
    "- [Part 2: Embeddings Notebook](notebooks/part2_embeddings.ipynb) (Coming Soon)\n",
    "- [Blog Post: How GPT Reads Your Words](your-blog-link) (Coming Soon)\n",
    "\n",
    "**Happy tokenizing!** 🎉\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Your Tokenization Experiment\n",
      "==================================================\n",
      "Text: 'Replace me with anything you want to explore!'\n",
      "\n",
      "SimpleTokenizer: 45 tokens\n",
      "BPETokenizer: 42 tokens\n",
      "\n",
      "🔍 Tokenization breakdown:\n",
      "\n",
      "SimpleTokenizer:\n",
      "============================================================\n",
      "TOKENIZATION VISUALIZATION\n",
      "============================================================\n",
      "Original: \"Replace me with anything you want to explore!\"\n",
      "Tokens:   ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '!']\n",
      "IDs:      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
      "Count:    45 tokens\n",
      "============================================================\n",
      "\n",
      "BPETokenizer:\n",
      "============================================================\n",
      "TOKENIZATION VISUALIZATION\n",
      "============================================================\n",
      "Original: \"Replace me with anything you want to explore!\"\n",
      "Tokens:   ['ID:32', 'ID:0', 'ID:12', 'ID:22', 'ID:18', 'ID:8', 'ID:10', 'ID:12', 'ID:32', 'ID:19', 'ID:12', 'ID:49', 'ID:15', 'ID:26', 'ID:14', 'ID:32', 'ID:8', 'ID:20', 'ID:31', 'ID:26', 'ID:14', 'ID:15', 'ID:20', 'ID:0', 'ID:32', 'ID:31', 'ID:21', 'ID:27', 'ID:49', 'ID:8', 'ID:20', 'ID:26', 'ID:55', 'ID:21', 'ID:32', 'ID:12', 'ID:30', 'ID:22', 'ID:34', 'ID:24', 'ID:12', 'ID:4']\n",
      "IDs:      [32, 0, 12, 22, 18, 8, 10, 12, 32, 19, 12, 49, 15, 26, 14, 32, 8, 20, 31, 26, 14, 15, 20, 0, 32, 31, 21, 27, 49, 8, 20, 26, 55, 21, 32, 12, 30, 22, 34, 24, 12, 4]\n",
      "Count:    42 tokens\n",
      "============================================================\n",
      "\n",
      "🎉 Experiment complete! Try different texts to see how tokenization changes!\n"
     ]
    }
   ],
   "source": [
    "# 🧩 Final Challenge: Your Own Tokenization Experiment\n",
    "# This is your playground! Try anything you want to explore\n",
    "\n",
    "from mini_gpt.utils import visualize_tokens\n",
    "\n",
    "# Replace this with your own text\n",
    "your_text = \"Replace me with anything you want to explore!\"\n",
    "\n",
    "print(\"🎯 Your Tokenization Experiment\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: '{your_text}'\")\n",
    "print()\n",
    "\n",
    "# Test with both tokenizers\n",
    "simple_tokens = tokenizer.encode(your_text)\n",
    "bpe_tokens = bpe_tokenizer.encode(your_text)\n",
    "\n",
    "print(f\"SimpleTokenizer: {len(simple_tokens)} tokens\")\n",
    "print(f\"BPETokenizer: {len(bpe_tokens)} tokens\")\n",
    "print()\n",
    "\n",
    "# Visualize the tokenization\n",
    "print(\"🔍 Tokenization breakdown:\")\n",
    "\n",
    "print(\"\\nSimpleTokenizer:\")\n",
    "visualize_tokens(your_text, tokenizer)\n",
    "\n",
    "print(\"\\nBPETokenizer:\")\n",
    "visualize_tokens(your_text, bpe_tokenizer)\n",
    "\n",
    "print(\"\\n🎉 Experiment complete! Try different texts to see how tokenization changes!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
